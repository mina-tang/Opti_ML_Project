{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader , Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "import nevergrad as ng\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import CNN_Simple,All_CNN_C,VAE,LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainManager:\n",
    "    def __init__(self,model,dataloader_train,dataloader_test,loss):\n",
    "        self.model = model\n",
    "        self.best_model = None\n",
    "        self.dataloader_train = dataloader_train\n",
    "        self.dataloader_test = dataloader_test\n",
    "        self.best_score = 1e9\n",
    "        self.loss = loss\n",
    "        self.epoch = 1\n",
    "        self.batch_nb = 0\n",
    "        self.max_batch = len(dataloader_train)\n",
    "        self.iteration = 0\n",
    "        self.output = []\n",
    "        self.best_output = []\n",
    "\n",
    "    def weights_updating(self,weights):\n",
    "        for n, layer in enumerate(self.model.parameters()):\n",
    "            layer.data = torch.from_numpy(weights[n]).to(dtype=torch.double)\n",
    "            \n",
    "    def evaluate(self):\n",
    "        total_loss = 0\n",
    "        for x , y in self.dataloader_test :\n",
    "            yhat = self.model(x)\n",
    "            loss = self.loss(yhat,y)\n",
    "            total_loss += loss.item()\n",
    "        total_loss /= len(self.dataloader_test)\n",
    "        return total_loss\n",
    "\n",
    "    def cost_function(self, parameters):\n",
    "        self.batch_nb = self.iteration%self.max_batch\n",
    "        load_params(self.model, torch.tensor(parameters, dtype=torch.double))\n",
    "        x,y = list(self.dataloader_train)[self.batch_nb][0], list(self.dataloader_train)[self.batch_nb][1]\n",
    "        predi = self.model(x)\n",
    "        loss  = self.loss(predi,y)\n",
    "\n",
    "        #if self.batch_nb == self.max_batch-1:   #evaluate test loss once per epoch\n",
    "        test_loss = self.evaluate()\n",
    "        if self.best_score > test_loss :\n",
    "            self.best_score = test_loss\n",
    "            self.best_model = copy.deepcopy(self.model)\n",
    "            \n",
    "        self.output.append(test_loss)\n",
    "        self.best_output.append(self.best_score)\n",
    "        print(f'epoch {self.epoch}, batch {self.batch_nb+1}; test loss function (cross entropy loss) : {test_loss}, best score : {self.best_score}')\n",
    "        self.iteration += 1\n",
    "        if self.iteration%self.max_batch == 0: self.epoch += 1\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_loss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE_loss, self).__init__()\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    def forward(self, preds, labels):\n",
    "        x, mean, logvar = preds\n",
    "        reproduction_loss = self.loss_fn(x, labels)\n",
    "        return reproduction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_params(model, param_tensor):\n",
    "    current_index = 0\n",
    "    for param in model.parameters():\n",
    "        param_length = param.numel()\n",
    "        #print(param_length, param.size())\n",
    "        param.data = param_tensor[current_index:current_index + param_length].reshape(param.size())\n",
    "        current_index += param_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P1 : simple CNN for classification of Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Lambda(lambda x: x.double()),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "training_set = torchvision.datasets.FashionMNIST(\"./data\", train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.FashionMNIST(\"./data\", train=False, transform=transform, download=True)\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=len(training_set)//10, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=len(validation_set)//10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_Simple().to(device)\n",
    "torch.save(model.state_dict(), 'models/CNN_Simple.pt')\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "restart here to test another optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('models/CNN_Simple.pt'))\n",
    "\n",
    "epochs = 1\n",
    "nb_batch = len(training_loader)\n",
    "trainer = TrainManager(model,training_loader,validation_loader,loss)\n",
    "fitness = trainer.cost_function\n",
    "\n",
    "# Compute number of parameters of the model + initialize parametrization\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "parametrization = ng.p.Array(shape=(num_params,))\n",
    "optimizer = ng.optimizers.NGOpt(parametrization=parametrization, budget=epochs*nb_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 1; test loss function (cross entropy loss) : 2.3025850929940463, best score : 2.3025850929940463\n",
      "epoch 1, batch 2; test loss function (cross entropy loss) : 88278.15726468261, best score : 2.3025850929940463\n",
      "epoch 1, batch 3; test loss function (cross entropy loss) : 36402.53673382646, best score : 2.3025850929940463\n",
      "epoch 1, batch 4; test loss function (cross entropy loss) : 19545.610165464546, best score : 2.3025850929940463\n",
      "epoch 1, batch 5; test loss function (cross entropy loss) : 8795.187136974575, best score : 2.3025850929940463\n",
      "epoch 1, batch 6; test loss function (cross entropy loss) : 6232.430953810803, best score : 2.3025850929940463\n",
      "epoch 1, batch 7; test loss function (cross entropy loss) : 2335.5201243622205, best score : 2.3025850929940463\n",
      "epoch 1, batch 8; test loss function (cross entropy loss) : 833.3223277934752, best score : 2.3025850929940463\n",
      "epoch 1, batch 9; test loss function (cross entropy loss) : 678.7597184136682, best score : 2.3025850929940463\n",
      "epoch 1, batch 10; test loss function (cross entropy loss) : 351.9627274786334, best score : 2.3025850929940463\n"
     ]
    }
   ],
   "source": [
    "learned_param = optimizer.minimize(fitness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P2: VAE for generation on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Lambda(lambda x: x.double())])\n",
    "training_set = torchvision.datasets.MNIST('./data', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.MNIST('./data', train=False, transform=transform, download=True)\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=64, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE().to(device)\n",
    "torch.save(model.state_dict(), 'models/VAE.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = VAE_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "restart here for different optimizer (NGOpt and SPSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('models/VAE.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "trainer = TrainManager(model,training_loader,validation_loader,loss)\n",
    "fitness = trainer.cost_function\n",
    "\n",
    "# Compute number of parameters of the model + initialize parametrization\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "parametrization = ng.p.Array(shape=(num_params,))\n",
    "optimizer = ng.optimizers.SPSA(parametrization=parametrization, budget=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 1; test loss function (cross entropy loss) : 6.6603685504075125, best score : 6.6603685504075125\n"
     ]
    }
   ],
   "source": [
    "learned_param = optimizer.minimize(fitness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P3: VAE for generation on MNIST (P2 on a different dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Lambda(lambda x: x.double()),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "training_set = torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=128, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE().to(device)\n",
    "loss = VAE_loss()\n",
    "torch.save(model.state_dict(), 'models/VAE.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "restart here for different optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('models/VAE.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "trainer = TrainManager(model,training_loader,validation_loader,loss)\n",
    "fitness = trainer.cost_function\n",
    "\n",
    "# Compute number of parameters of the model + initialize parametrization\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "parametrization = ng.p.Array(shape=(num_params,))\n",
    "optimizer = ng.optimizers.SPSA(parametrization=parametrization, budget=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 1; test loss function (cross entropy loss) : 6.651875184703448, best score : 6.651875184703448\n"
     ]
    }
   ],
   "source": [
    "learned_param = optimizer.minimize(fitness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P4: All-CNNC-C for classification on CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.double()),\n",
    "    transforms.Normalize((0.5074,0.4867,0.4411),(0.2011,0.1987,0.2025))]\n",
    ")\n",
    "\n",
    "training_set = torchvision.datasets.CIFAR100('./data', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.CIFAR100('./data', train=False, transform=transform, download=True)\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=len(training_set)//1000, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=len(validation_set)//1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = All_CNN_C().to(device)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "torch.save(model.state_dict(), 'models/All_CNN_C.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "restart here for evaluating with different optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('models/All_CNN_C.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "trainer = TrainManager(model,training_loader,validation_loader,loss)\n",
    "fitness = trainer.cost_function\n",
    "\n",
    "# Compute number of parameters of the model + initialize parametrization\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "parametrization = ng.p.Array(shape=(num_params,))\n",
    "optimizer = ng.optimizers.SPSA(parametrization=parametrization, budget=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 1; test loss function (cross entropy loss) : 752.5132519449395, best score : 752.5132519449395\n"
     ]
    }
   ],
   "source": [
    "learned_param = optimizer.minimize(fitness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P5: next word generation (2-layer bidirectional LSTM trained on wikitext-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"wikitext\", 'wikitext-2-v1', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(sample):\n",
    "    try:\n",
    "        sample.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "def lowerCase(sample):\n",
    "    return {\"text\": sample[\"text\"].lower()}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "dataset = dataset.filter(lambda x: 100 <= len(x['text'].split()) <= 128)\n",
    "dataset = dataset.filter(lambda x: not re.match(\" = .* = \\n\", x['text']))\n",
    "dataset = dataset.filter(lambda x: isEnglish(x['text']))\n",
    "dataset = dataset.map(lambda x: lowerCase(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def count_tokens(dataset):\n",
    "    \"\"\"Counts the frequency of each token in the dataset.\n",
    "    return a dict with token as keys, frequency as values.\"\"\"\n",
    "\n",
    "    token_freq_dict = Counter(\" \".join((x['text'] for x in dataset)).split())\n",
    "    return token_freq_dict\n",
    "\n",
    "def replace_rare_tokens(sample, rare_tokens, unk_token):\n",
    "    text = sample[\"text\"]\n",
    "    modified_tokens = [(token if token not in rare_tokens else unk_token)\n",
    "                       for token in text.split()]\n",
    "    return {\"text\": \" \".join(modified_tokens)}\n",
    "\n",
    "def is_unknown_sequence(sample, unk_token, unk_threshold=0.1):\n",
    "    sample_tokens = sample[\"text\"].split()\n",
    "    if sample_tokens.count(unk_token)/len(sample_tokens) > unk_threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def build_vocabulary(dataset, min_freq=5, unk_token='<unk>'):\n",
    "    \"\"\"Builds a vocabulary dict for the given dataset.\"\"\"\n",
    "    # Get unique tokens and their frequencies.\n",
    "    token_freq_dict = count_tokens(dataset)\n",
    "\n",
    "    # Find a set of rare tokens with frequency lower than `min_freq` and replace them with `unk_token`.\n",
    "    rare_tokens_set = set()\n",
    "    low_freq = [x[0] for x in token_freq_dict.items() if x[1] <= min_freq]\n",
    "    rare_tokens_set.update(low_freq)\n",
    "    dataset = dataset.map(replace_rare_tokens, fn_kwargs={\"rare_tokens\": rare_tokens_set,\n",
    "                                                  \"unk_token\": unk_token})\n",
    "\n",
    "    # Filter out sequences with more than 15% rare tokens.\n",
    "    dataset = dataset.filter(lambda x: not is_unknown_sequence(x, unk_token, unk_threshold=0.15))\n",
    "\n",
    "    # Recompute the token frequency to get final vocabulary dict.\n",
    "    token_freq_dict = count_tokens(dataset)\n",
    "    return dataset, token_freq_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikitext_dataset, token_freq_dict = build_vocabulary(dataset, min_freq=5, unk_token='<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 dataset: datasets.arrow_dataset.Dataset,\n",
    "                 max_seq_length: int, ):\n",
    "        self.train_data = self.prepare_dataset(dataset)\n",
    "        self.max_seq_length = max_seq_length + 2  # as <start> and <stop> will be added\n",
    "        self.dataset_vocab = self.get_vocabulary(dataset)\n",
    "        self.token2idx = {element: index for index, element in enumerate(self.dataset_vocab)}\n",
    "        self.idx2token = dict(enumerate(self.dataset_vocab))\n",
    "        self.pad_idx = self.token2idx[\"<pad>\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get a list of tokens of the given sequence. Represent each token with its index in `self.token2idx`.\n",
    "        token_list = self.train_data[idx].split()\n",
    "        # having a fallback to <unk> token if an unseen word is encoded.\n",
    "        token_ids = [self.token2idx.get(t, self.token2idx['<unk>']) for t in token_list]\n",
    "\n",
    "        # Add padding token to the sequence to reach the max_seq_length. \n",
    "        token_ids += [self.token2idx['<pad>']] * (self.max_seq_length - len(token_ids))\n",
    "\n",
    "        return torch.tensor(token_ids)\n",
    "\n",
    "    def get_vocabulary(self, dataset: datasets.arrow_dataset.Dataset):\n",
    "        vocab = set()\n",
    "        print(\"Getting dataset's vocabulary\")\n",
    "        for sample in tqdm(dataset):\n",
    "            vocab.update(set(sample[\"text\"].split()))\n",
    "        vocab.update(set([\"<start>\", \"<stop>\", \"<pad>\"]))\n",
    "        vocab = sorted(vocab)\n",
    "        return vocab\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_dataset(target_dataset: datasets.arrow_dataset.Dataset):\n",
    "        \"\"\"\n",
    "        Encapsulate sequences between <start> and <stop>.\n",
    "        \n",
    "        :param: target_dataset: the target dataset to extract samples\n",
    "        return: a list of encapsulated samples.\n",
    "        \"\"\"\n",
    "        prepared_dataset = []\n",
    "        for sample in target_dataset:\n",
    "            prepared_dataset.append(f\"<start> {sample['text']} <stop>\")\n",
    "        return prepared_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting dataset's vocabulary\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m MAX_SEQ_LENGTH \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[1;32m----> 2\u001b[0m lstm_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mLSTMDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwikitext_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_SEQ_LENGTH\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[41], line 7\u001b[0m, in \u001b[0;36mLSTMDataset.__init__\u001b[1;34m(self, dataset, max_seq_length)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_dataset(dataset)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length \u001b[38;5;241m=\u001b[39m max_seq_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# as <start> and <stop> will be added\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_vocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vocabulary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken2idx \u001b[38;5;241m=\u001b[39m {element: index \u001b[38;5;28;01mfor\u001b[39;00m index, element \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_vocab)}\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx2token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_vocab))\n",
      "Cell \u001b[1;32mIn[41], line 29\u001b[0m, in \u001b[0;36mLSTMDataset.get_vocabulary\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m     27\u001b[0m vocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGetting dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms vocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     30\u001b[0m     vocab\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mset\u001b[39m(sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msplit()))\n\u001b[0;32m     31\u001b[0m vocab\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mset\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<start>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<stop>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LENGTH = 128\n",
    "lstm_dataset = LSTMDataset(dataset=wikitext_dataset,\n",
    "                         max_seq_length=MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(lstm_dataset, test_ratio=0.1):\n",
    "    # split train/test dataset.\n",
    "    lstm_train_dataset, lstm_test_dataset = torch.utils.data.random_split(lstm_dataset, [1-test_ratio, test_ratio])\n",
    "    # get pytorch DataLoader\n",
    "    train_dataloader = DataLoader(lstm_train_dataset, batch_size=8, shuffle=True)\n",
    "    test_dataloader = DataLoader(lstm_test_dataset, batch_size=8, shuffle=False)\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lstm_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_loader, val_loader \u001b[38;5;241m=\u001b[39m get_dataloader(\u001b[43mlstm_dataset\u001b[49m, test_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lstm_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = get_dataloader(lstm_dataset, test_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(lstm_dataset.token2idx)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 100\n",
    "dropout_rate = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(vocab_size=vocab_size,input_dim=embedding_dim,hidden_dim=hidden_dim,dropout_rate=dropout_rate).to(device)\n",
    "loss = torch.nn.CrossEntropyLoss(ignore_index=lstm_dataset.pad_idx)\n",
    "torch.save(model.state_dict(), 'models/LSTM.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "restart here for evaluating different optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('models/LSTM.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "trainer = TrainManager(model,training_loader,validation_loader,loss)\n",
    "fitness = trainer.cost_function\n",
    "\n",
    "# Compute number of parameters of the model + initialize parametrization\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "parametrization = ng.p.Array(shape=(num_params,))\n",
    "optimizer = ng.optimizers.SPSA(parametrization=parametrization, budget=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_param = optimizer.minimize(fitness)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opti4ML",
   "language": "python",
   "name": "opti4ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

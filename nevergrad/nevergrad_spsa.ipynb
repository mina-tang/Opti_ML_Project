{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader , Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "import nevergrad as ng\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"nevergrad/\")\n",
    "\n",
    "\n",
    "\n",
    "import models\n",
    "from models import CNN_Simple as CNN_Simple\n",
    "from models import All_CNN_C as All_CNN_C\n",
    "from models import VAE as VAE\n",
    "from models import LSTM as LSTM\n",
    "\n",
    "\n",
    "from pytorch_optim_training_manager import train_manager\n",
    "import torch\n",
    "import torchvision\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import models\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Lambda(lambda x: x.double()),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "training_set = torchvision.datasets.FashionMNIST(\"./data\", train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.FashionMNIST(\"./data\", train=False, transform=transform, download=True)\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=64, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_params(model, param_tensor):\n",
    "    current_index = 0\n",
    "    for param in model.parameters():\n",
    "        param_length = param.numel()\n",
    "        #print(param_length, param.size())\n",
    "        param.data = param_tensor[current_index:current_index + param_length].reshape(param.size())\n",
    "        current_index += param_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainManager:\n",
    "    def __init__(self,model,dataloader_train,dataloader_test,loss,output):\n",
    "        self.model = model\n",
    "        self.best_model = None\n",
    "        self.dataloader_train = dataloader_train\n",
    "        self.dataloader_test = dataloader_test\n",
    "        self.nb_batch = len(dataloader_train)\n",
    "        self.best_score = 1e9\n",
    "        self.loss = loss\n",
    "        self.iteration = 0\n",
    "        self.output = output\n",
    "        self.best_output = output\n",
    "        #self.best_accuracy = 0\n",
    "    \n",
    "    def weights_updating(self,weights):\n",
    "        for n, layer in enumerate(self.model.parameters()):\n",
    "            layer.data = torch.from_numpy(weights[n]).to(dtype=torch.double)\n",
    "            \n",
    "    def evaluate(self):\n",
    "        #correct = 0\n",
    "        total_loss = 0\n",
    "        for x , y in self.dataloader_test :\n",
    "            yhat = self.model(x)\n",
    "            #print(\"yhat\", yhat,\"y\", y)\n",
    "            #correct += (torch.abs(yhat-y)<0.5).type(torch.double).sum().item() # Compute classification error\n",
    "            loss = self.loss(yhat,y)\n",
    "            total_loss += loss.item()\n",
    "        #correct /= len(self.dataloader_test.dataset)\n",
    "        total_loss /= len(self.dataloader_test)\n",
    "        return total_loss#,correct\n",
    "\n",
    "    def cost_function(self, parameters):\n",
    "        load_params(self.model, torch.tensor(parameters, dtype=torch.double))\n",
    "\n",
    "        test_loss = self.evaluate() #, accuracy = self.evaluate()\n",
    "        if self.best_score > test_loss :\n",
    "            self.best_score = test_loss\n",
    "            self.best_model = copy.deepcopy(self.model)\n",
    "        #if self.best_accuracy < accuracy :\n",
    "        #    self.best_accuracy = accuracy\n",
    "        #    self.best_model = copy.deepcopy(self.model)\n",
    "\n",
    "        print(f'test loss function (crossentropyloss) : {test_loss}, best score : {self.best_score}') #precision : {(100*accuracy):>0.1f}% ,\n",
    "        self.output[self.iteration,:] = np.array([test_loss])\n",
    "        self.best_output[self.iteration,:] = np.array([self.best_score])\n",
    "        self.iteration += 1\n",
    "        return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN_Simple().to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "model.load_state_dict(torch.load('models/CNN_Simple.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "output = np.empty((epochs,2))\n",
    "\n",
    "trainer = TrainManager(model,training_loader,validation_loader,loss,output)\n",
    "fitness = trainer.cost_function\n",
    "\n",
    "# Compute number of parameters of the model + initialize parametrization\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "parametrization = ng.p.Array(shape=(num_params,))\n",
    "optimizer = ng.optimizers.SPSA(parametrization=parametrization, budget=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss function (mse) : 2.358402514401358, best score : 2.358402514401358\n",
      "test loss function (mse) : 2.358886341232512, best score : 2.358402514401358\n",
      "test loss function (mse) : 2.302994156365421, best score : 2.302994156365421\n",
      "test loss function (mse) : 2.322891583824568, best score : 2.302994156365421\n",
      "test loss function (mse) : 2.2989695839314948, best score : 2.2989695839314948\n",
      "test loss function (mse) : 2.310800665366812, best score : 2.2989695839314948\n",
      "test loss function (mse) : 2.3574915454361394, best score : 2.2989695839314948\n",
      "test loss function (mse) : 2.299661727711368, best score : 2.2989695839314948\n",
      "test loss function (mse) : 2.3560911883034836, best score : 2.2989695839314948\n",
      "test loss function (mse) : 2.3245422305659735, best score : 2.2989695839314948\n",
      "test loss function (mse) : 2.370400631278531, best score : 2.2989695839314948\n",
      "test loss function (mse) : 2.3322889634747503, best score : 2.2989695839314948\n",
      "test loss function (mse) : 2.3266613244104177, best score : 2.2989695839314948\n",
      "test loss function (mse) : 2.322842141784757, best score : 2.2989695839314948\n",
      "test loss function (mse) : 2.3166311540939675, best score : 2.2989695839314948\n",
      "test loss function (mse) : 2.3089399428052433, best score : 2.2989695839314948\n",
      "test loss function (mse) : 2.316940569671323, best score : 2.2989695839314948\n",
      "test loss function (mse) : 2.3336044134005847, best score : 2.2989695839314948\n",
      "test loss function (mse) : 2.311056186710668, best score : 2.2989695839314948\n",
      "test loss function (mse) : 2.3388960439304216, best score : 2.2989695839314948\n"
     ]
    }
   ],
   "source": [
    "learned_param = optimizer.minimize(fitness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Lambda(lambda x: x.double())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = torchvision.datasets.MNIST('./data', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.MNIST('./data', train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=64, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.VAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/VAE.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_loss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE_loss, self).__init__()\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    def forward(self, preds, labels):\n",
    "        x, mean, logvar = preds\n",
    "        reproduction_loss = self.loss_fn(x, labels)\n",
    "        return reproduction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = VAE_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart here for a different optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('models/VAE.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizer = ng.optimizers.SPSA(parametrization=parametrization, budget=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "\n",
    "output = np.empty((epochs,2))\n",
    "\n",
    "trainer = TrainManager(model,training_loader,validation_loader,loss,output)\n",
    "fitness = trainer.cost_function\n",
    "\n",
    "# Compute number of parameters of the model + initialize parametrization\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "parametrization = ng.p.Array(shape=(num_params,))\n",
    "optimizer = ng.optimizers.SPSA(parametrization=parametrization, budget=epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss function (mse) : 6.661176861385032, best score : 6.661176861385032\n",
      "test loss function (mse) : 6.6712369771543765, best score : 6.661176861385032\n",
      "test loss function (mse) : 6.66504180332468, best score : 6.661176861385032\n",
      "test loss function (mse) : 6.667532498200626, best score : 6.661176861385032\n",
      "test loss function (mse) : 6.66346659006216, best score : 6.661176861385032\n",
      "test loss function (mse) : 6.665673173967799, best score : 6.661176861385032\n",
      "test loss function (mse) : 6.659545614241042, best score : 6.659545614241042\n",
      "test loss function (mse) : 6.679408236837802, best score : 6.659545614241042\n",
      "test loss function (mse) : 6.669745445928182, best score : 6.659545614241042\n",
      "test loss function (mse) : 6.647554329206105, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.664105181089007, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.664643962723846, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.657447295130584, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.682100383951393, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.668795575526855, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.681262749630405, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.666873057469777, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.663003744262355, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.658616132622908, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.66192729922674, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.661138891545277, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.6571575816941015, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.666605148233134, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.6698122381943294, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.663929666346386, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.658562770758218, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.671362388446991, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.667857762967048, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.672309354041186, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.667719312209953, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.656031200391123, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.654959337898356, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.665176009076204, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.665845883210654, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.66478966088877, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.666452315576929, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.666135242634069, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.650550596054635, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.656219584002449, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.665671205363941, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.6790737635584385, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.661346452872088, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.668120429325014, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.670202844615398, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.656066610838333, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.669493116145746, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.656684813523743, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.661111095849294, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.67038302244808, best score : 6.647554329206105\n",
      "test loss function (mse) : 6.667796833277381, best score : 6.647554329206105\n"
     ]
    }
   ],
   "source": [
    "learned_param = optimizer.minimize(fitness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Lambda(lambda x: x.double()),\n",
    "    transforms.Normalize((0.5,), (0.5,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=128, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.VAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = VAE_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/VAE.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart here for different optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('models/VAE.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = ng.optimizers.SPSA(parametrization=parametrization, budget=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "output = np.empty((epochs,2))\n",
    "\n",
    "trainer = TrainManager(model,training_loader,validation_loader,loss,output)\n",
    "fitness = trainer.cost_function\n",
    "\n",
    "# Compute number of parameters of the model + initialize parametrization\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "parametrization = ng.p.Array(shape=(num_params,))\n",
    "optimizer = ng.optimizers.SPSA(parametrization=parametrization, budget=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss function (mse) : 6.642322217255193, best score : 6.642322217255193\n",
      "test loss function (mse) : 6.664957735035388, best score : 6.642322217255193\n",
      "test loss function (mse) : 6.660617371469891, best score : 6.642322217255193\n",
      "test loss function (mse) : 6.644525445492171, best score : 6.642322217255193\n",
      "test loss function (mse) : 6.660206301097731, best score : 6.642322217255193\n",
      "test loss function (mse) : 6.673321846232124, best score : 6.642322217255193\n",
      "test loss function (mse) : 6.655907959338862, best score : 6.642322217255193\n",
      "test loss function (mse) : 6.671545413658366, best score : 6.642322217255193\n",
      "test loss function (mse) : 6.685117846175073, best score : 6.642322217255193\n",
      "test loss function (mse) : 6.650063830461741, best score : 6.642322217255193\n",
      "test loss function (mse) : 6.6715362896263155, best score : 6.642322217255193\n",
      "test loss function (mse) : 6.653201306122378, best score : 6.642322217255193\n",
      "test loss function (mse) : 6.666598537193816, best score : 6.642322217255193\n",
      "test loss function (mse) : 6.663409851203459, best score : 6.642322217255193\n",
      "test loss function (mse) : 6.660654466770933, best score : 6.642322217255193\n",
      "test loss function (mse) : 6.653912491497102, best score : 6.642322217255193\n",
      "test loss function (mse) : 6.662580224552374, best score : 6.642322217255193\n",
      "test loss function (mse) : 6.660990903918342, best score : 6.642322217255193\n",
      "test loss function (mse) : 6.661964726367268, best score : 6.642322217255193\n",
      "test loss function (mse) : 6.6618944377638645, best score : 6.642322217255193\n"
     ]
    }
   ],
   "source": [
    "learned_param = optimizer.minimize(fitness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Lambda(lambda x: x.double()),\n",
    "    transforms.Normalize((0.5,), (0.5,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169001437/169001437 [00:06<00:00, 26284216.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "training_set = torchvision.datasets.CIFAR100('./data', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.CIFAR100('./data', train=False, transform=transform, download=True)\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=256, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.All_CNN_C()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/All_CNN_C.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "restart here if evaluating multiple optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('models/All_CNN_C.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "output = np.empty((epochs,2))\n",
    "\n",
    "trainer = TrainManager(model,training_loader,validation_loader,loss,output)\n",
    "fitness = trainer.cost_function\n",
    "\n",
    "# Compute number of parameters of the model + initialize parametrization\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "parametrization = ng.p.Array(shape=(num_params,))\n",
    "optimizer = ng.optimizers.SPSA(parametrization=parametrization, budget=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss function (mse) : 259.0143385284827, best score : 259.0143385284827\n",
      "test loss function (mse) : 250.35841448954207, best score : 250.35841448954207\n",
      "test loss function (mse) : 181.76296058976112, best score : 181.76296058976112\n",
      "test loss function (mse) : 206.09000366009255, best score : 181.76296058976112\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[111], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m learned_param \u001B[38;5;241m=\u001B[39m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mminimize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfitness\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.venv/lib/python3.12/site-packages/nevergrad/optimization/base.py:678\u001B[0m, in \u001B[0;36mOptimizer.minimize\u001B[0;34m(self, objective_function, executor, batch_mode, verbosity, constraint_violation)\u001B[0m\n\u001B[1;32m    676\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_finished_jobs:\n\u001B[1;32m    677\u001B[0m     x, job \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_finished_jobs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m--> 678\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mjob\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    679\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m constraint_violation \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    680\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtell(\n\u001B[1;32m    681\u001B[0m             x, result, [f(x\u001B[38;5;241m.\u001B[39mvalue) \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m constraint_violation]\n\u001B[1;32m    682\u001B[0m         )  \u001B[38;5;66;03m# TODO: this is not parallelized, wtf!\u001B[39;00m\n",
      "File \u001B[0;32m~/.venv/lib/python3.12/site-packages/nevergrad/optimization/utils.py:145\u001B[0m, in \u001B[0;36mDelayedJob.result\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mresult\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m tp\u001B[38;5;241m.\u001B[39mAny:\n\u001B[1;32m    144\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_computed:\n\u001B[0;32m--> 145\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    146\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_computed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    147\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result\n",
      "Cell \u001B[0;32mIn[75], line 35\u001B[0m, in \u001B[0;36mTrainManager.cost_function\u001B[0;34m(self, parameters)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcost_function\u001B[39m(\u001B[38;5;28mself\u001B[39m, parameters):\n\u001B[1;32m     33\u001B[0m     load_params(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, torch\u001B[38;5;241m.\u001B[39mtensor(parameters, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdouble))\n\u001B[0;32m---> 35\u001B[0m     test_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m#, accuracy = self.evaluate()\u001B[39;00m\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_score \u001B[38;5;241m>\u001B[39m test_loss :\n\u001B[1;32m     37\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_score \u001B[38;5;241m=\u001B[39m test_loss\n",
      "Cell \u001B[0;32mIn[75], line 23\u001B[0m, in \u001B[0;36mTrainManager.evaluate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     21\u001B[0m total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m x , y \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataloader_test :\n\u001B[0;32m---> 23\u001B[0m     yhat \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;66;03m#print(\"yhat\", yhat,\"y\", y)\u001B[39;00m\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;66;03m#correct += (torch.abs(yhat-y)<0.5).type(torch.double).sum().item() # Compute classification error\u001B[39;00m\n\u001B[1;32m     26\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss(yhat,y)\n",
      "File \u001B[0;32m~/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Opti_ML_Project/nevergrad/../models.py:100\u001B[0m, in \u001B[0;36mAll_CNN_C.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     98\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdp(x)\n\u001B[1;32m     99\u001B[0m x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv4(x))\n\u001B[0;32m--> 100\u001B[0m x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv5\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    101\u001B[0m x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv6(x))\n\u001B[1;32m    102\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdp(x)\n",
      "File \u001B[0;32m~/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:460\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    459\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 460\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:456\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    452\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    453\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[1;32m    454\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    455\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[0;32m--> 456\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    457\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "learned_param = optimizer.minimize(fitness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.double()),\n",
    "    transforms.Normalize((0.5074,0.4867,0.4411),(0.2011,0.1987,0.2025))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "training_set = torchvision.datasets.CIFAR100('./data', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.CIFAR100('./data', train=False, transform=transform, download=True)\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=256, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.All_CNN_C()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/All_CNN_C.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "restart here if evaluating multiple optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('models/All_CNN_C.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "output = np.empty((epochs,2))\n",
    "\n",
    "trainer = TrainManager(model,training_loader,validation_loader,loss,output)\n",
    "fitness = trainer.cost_function\n",
    "\n",
    "# Compute number of parameters of the model + initialize parametrization\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "parametrization = ng.p.Array(shape=(num_params,))\n",
    "optimizer = ng.optimizers.SPSA(parametrization=parametrization, budget=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss function (mse) : 548.0120398875177, best score : 548.0120398875177\n",
      "test loss function (mse) : 349.257397669483, best score : 349.257397669483\n",
      "test loss function (mse) : 275.35710093303993, best score : 275.35710093303993\n",
      "test loss function (mse) : 207.66149525968973, best score : 207.66149525968973\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[119], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m learned_param \u001B[38;5;241m=\u001B[39m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mminimize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfitness\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.venv/lib/python3.12/site-packages/nevergrad/optimization/base.py:678\u001B[0m, in \u001B[0;36mOptimizer.minimize\u001B[0;34m(self, objective_function, executor, batch_mode, verbosity, constraint_violation)\u001B[0m\n\u001B[1;32m    676\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_finished_jobs:\n\u001B[1;32m    677\u001B[0m     x, job \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_finished_jobs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m--> 678\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mjob\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    679\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m constraint_violation \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    680\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtell(\n\u001B[1;32m    681\u001B[0m             x, result, [f(x\u001B[38;5;241m.\u001B[39mvalue) \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m constraint_violation]\n\u001B[1;32m    682\u001B[0m         )  \u001B[38;5;66;03m# TODO: this is not parallelized, wtf!\u001B[39;00m\n",
      "File \u001B[0;32m~/.venv/lib/python3.12/site-packages/nevergrad/optimization/utils.py:145\u001B[0m, in \u001B[0;36mDelayedJob.result\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mresult\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m tp\u001B[38;5;241m.\u001B[39mAny:\n\u001B[1;32m    144\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_computed:\n\u001B[0;32m--> 145\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    146\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_computed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    147\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result\n",
      "Cell \u001B[0;32mIn[75], line 35\u001B[0m, in \u001B[0;36mTrainManager.cost_function\u001B[0;34m(self, parameters)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcost_function\u001B[39m(\u001B[38;5;28mself\u001B[39m, parameters):\n\u001B[1;32m     33\u001B[0m     load_params(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, torch\u001B[38;5;241m.\u001B[39mtensor(parameters, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdouble))\n\u001B[0;32m---> 35\u001B[0m     test_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m#, accuracy = self.evaluate()\u001B[39;00m\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_score \u001B[38;5;241m>\u001B[39m test_loss :\n\u001B[1;32m     37\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_score \u001B[38;5;241m=\u001B[39m test_loss\n",
      "Cell \u001B[0;32mIn[75], line 23\u001B[0m, in \u001B[0;36mTrainManager.evaluate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     21\u001B[0m total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m x , y \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataloader_test :\n\u001B[0;32m---> 23\u001B[0m     yhat \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;66;03m#print(\"yhat\", yhat,\"y\", y)\u001B[39;00m\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;66;03m#correct += (torch.abs(yhat-y)<0.5).type(torch.double).sum().item() # Compute classification error\u001B[39;00m\n\u001B[1;32m     26\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss(yhat,y)\n",
      "File \u001B[0;32m~/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Opti_ML_Project/nevergrad/../models.py:99\u001B[0m, in \u001B[0;36mAll_CNN_C.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     97\u001B[0m x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv3(x))\n\u001B[1;32m     98\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdp(x)\n\u001B[0;32m---> 99\u001B[0m x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv4\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    100\u001B[0m x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv5(x))\n\u001B[1;32m    101\u001B[0m x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv6(x))\n",
      "File \u001B[0;32m~/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:460\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    459\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 460\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:456\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    452\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    453\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[1;32m    454\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    455\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[0;32m--> 456\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    457\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "learned_param = optimizer.minimize(fitness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/m/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|██████████| 10.5k/10.5k [00:00<00:00, 30.7MB/s]\n",
      "Downloading data: 100%|██████████| 685k/685k [00:00<00:00, 1.60MB/s]\n",
      "Downloading data: 100%|██████████| 6.07M/6.07M [00:00<00:00, 13.6MB/s]\n",
      "Downloading data: 100%|██████████| 618k/618k [00:00<00:00, 2.08MB/s]\n",
      "Generating test split: 100%|██████████| 4358/4358 [00:00<00:00, 149322.99 examples/s]\n",
      "Generating train split: 100%|██████████| 36718/36718 [00:00<00:00, 2107541.04 examples/s]\n",
      "Generating validation split: 100%|██████████| 3760/3760 [00:00<00:00, 1699050.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"wikitext\", 'wikitext-2-v1', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(sample):\n",
    "    try:\n",
    "        sample.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "def lowerCase(sample):\n",
    "    return {\"text\": sample[\"text\"].lower()}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 2668/2668 [00:00<00:00, 87785.77 examples/s]\n",
      "Filter: 100%|██████████| 2668/2668 [00:00<00:00, 226852.42 examples/s]\n",
      "Filter: 100%|██████████| 2668/2668 [00:00<00:00, 215162.82 examples/s]\n",
      "Map: 100%|██████████| 1972/1972 [00:00<00:00, 47082.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "dataset = dataset.filter(lambda x: 100 <= len(x['text'].split()) <= 128)\n",
    "dataset = dataset.filter(lambda x: not re.match(\" = .* = \\n\", x['text']))\n",
    "dataset = dataset.filter(lambda x: isEnglish(x['text']))\n",
    "dataset = dataset.map(lambda x: lowerCase(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def count_tokens(dataset):\n",
    "    \"\"\"Counts the frequency of each token in the dataset.\n",
    "    return a dict with token as keys, frequency as values.\"\"\"\n",
    "\n",
    "    token_freq_dict = Counter(\" \".join((x['text'] for x in dataset)).split())\n",
    "    return token_freq_dict\n",
    "\n",
    "def replace_rare_tokens(sample, rare_tokens, unk_token):\n",
    "    text = sample[\"text\"]\n",
    "    modified_tokens = [(token if token not in rare_tokens else unk_token)\n",
    "                       for token in text.split()]\n",
    "    return {\"text\": \" \".join(modified_tokens)}\n",
    "\n",
    "def is_unknown_sequence(sample, unk_token, unk_threshold=0.1):\n",
    "    sample_tokens = sample[\"text\"].split()\n",
    "    if sample_tokens.count(unk_token)/len(sample_tokens) > unk_threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def build_vocabulary(dataset, min_freq=5, unk_token='<unk>'):\n",
    "    \"\"\"Builds a vocabulary dict for the given dataset.\"\"\"\n",
    "    # Get unique tokens and their frequencies.\n",
    "    token_freq_dict = count_tokens(dataset)\n",
    "\n",
    "    # Find a set of rare tokens with frequency lower than `min_freq` and replace them with `unk_token`.\n",
    "    rare_tokens_set = set()\n",
    "    low_freq = [x[0] for x in token_freq_dict.items() if x[1] <= min_freq]\n",
    "    rare_tokens_set.update(low_freq)\n",
    "    dataset = dataset.map(replace_rare_tokens, fn_kwargs={\"rare_tokens\": rare_tokens_set,\n",
    "                                                  \"unk_token\": unk_token})\n",
    "\n",
    "    # Filter out sequences with more than 15% rare tokens.\n",
    "    dataset = dataset.filter(lambda x: not is_unknown_sequence(x, unk_token, unk_threshold=0.15))\n",
    "\n",
    "    # Recompute the token frequency to get final vocabulary dict.\n",
    "    token_freq_dict = count_tokens(dataset)\n",
    "    return dataset, token_freq_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1972/1972 [00:00<00:00, 43588.67 examples/s]\n",
      "Filter: 100%|██████████| 1972/1972 [00:00<00:00, 163584.66 examples/s]\n"
     ]
    }
   ],
   "source": [
    "wikitext_dataset, token_freq_dict = build_vocabulary(dataset, min_freq=5, unk_token='<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 dataset: datasets.arrow_dataset.Dataset,\n",
    "                 max_seq_length: int, ):\n",
    "        self.train_data = self.prepare_dataset(dataset)\n",
    "        self.max_seq_length = max_seq_length + 2  # as <start> and <stop> will be added\n",
    "        self.dataset_vocab = self.get_vocabulary(dataset)\n",
    "        self.token2idx = {element: index for index, element in enumerate(self.dataset_vocab)}\n",
    "        self.idx2token = dict(enumerate(self.dataset_vocab))\n",
    "        self.pad_idx = self.token2idx[\"<pad>\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get a list of tokens of the given sequence. Represent each token with its index in `self.token2idx`.\n",
    "        token_list = self.train_data[idx].split()\n",
    "        # having a fallback to <unk> token if an unseen word is encoded.\n",
    "        token_ids = [self.token2idx.get(t, self.token2idx['<unk>']) for t in token_list]\n",
    "\n",
    "        # Add padding token to the sequence to reach the max_seq_length. \n",
    "        token_ids += [self.token2idx['<pad>']] * (self.max_seq_length - len(token_ids))\n",
    "\n",
    "        return torch.tensor(token_ids)\n",
    "\n",
    "    def get_vocabulary(self, dataset: datasets.arrow_dataset.Dataset):\n",
    "        vocab = set()\n",
    "        print(\"Getting dataset's vocabulary\")\n",
    "        for sample in tqdm(dataset):\n",
    "            vocab.update(set(sample[\"text\"].split()))\n",
    "        vocab.update(set([\"<start>\", \"<stop>\", \"<pad>\"]))\n",
    "        vocab = sorted(vocab)\n",
    "        return vocab\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_dataset(target_dataset: datasets.arrow_dataset.Dataset):\n",
    "        \"\"\"\n",
    "        Encapsulate sequences between <start> and <stop>.\n",
    "        \n",
    "        :param: target_dataset: the target dataset to extract samples\n",
    "        return: a list of encapsulated samples.\n",
    "        \"\"\"\n",
    "        prepared_dataset = []\n",
    "        for sample in target_dataset:\n",
    "            prepared_dataset.append(f\"<start> {sample['text']} <stop>\")\n",
    "        return prepared_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting dataset's vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1158/1158 [00:00<00:00, 33723.57it/s]\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LENGTH = 128\n",
    "lstm_dataset = LSTMDataset(dataset=wikitext_dataset,\n",
    "                         max_seq_length=MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(lstm_dataset, test_ratio=0.1):\n",
    "    # split train/test dataset.\n",
    "    lstm_train_dataset, lstm_test_dataset = torch.utils.data.random_split(lstm_dataset, [1-test_ratio, test_ratio])\n",
    "    # get pytorch DataLoader\n",
    "    train_dataloader = DataLoader(lstm_train_dataset, batch_size=8, shuffle=True)\n",
    "    test_dataloader = DataLoader(lstm_test_dataset, batch_size=8, shuffle=False)\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = get_dataloader(lstm_dataset, test_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(lstm_dataset.token2idx)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 100\n",
    "dropout_rate = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.LSTM(vocab_size=vocab_size,input_dim=embedding_dim,hidden_dim=hidden_dim,dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss(ignore_index=lstm_dataset.pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/LSTM.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart here for evaluating multiple optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('models/LSTM.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "output = np.empty((epochs,2))\n",
    "\n",
    "trainer = TrainManager(model,training_loader,validation_loader,loss,output)\n",
    "fitness = trainer.cost_function\n",
    "\n",
    "# Compute number of parameters of the model + initialize parametrization\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "parametrization = ng.p.Array(shape=(num_params,))\n",
    "optimizer = ng.optimizers.SPSA(parametrization=parametrization, budget=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.DoubleTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[144], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m learned_param \u001B[38;5;241m=\u001B[39m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mminimize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfitness\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.venv/lib/python3.12/site-packages/nevergrad/optimization/base.py:678\u001B[0m, in \u001B[0;36mOptimizer.minimize\u001B[0;34m(self, objective_function, executor, batch_mode, verbosity, constraint_violation)\u001B[0m\n\u001B[1;32m    676\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_finished_jobs:\n\u001B[1;32m    677\u001B[0m     x, job \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_finished_jobs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m--> 678\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mjob\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    679\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m constraint_violation \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    680\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtell(\n\u001B[1;32m    681\u001B[0m             x, result, [f(x\u001B[38;5;241m.\u001B[39mvalue) \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m constraint_violation]\n\u001B[1;32m    682\u001B[0m         )  \u001B[38;5;66;03m# TODO: this is not parallelized, wtf!\u001B[39;00m\n",
      "File \u001B[0;32m~/.venv/lib/python3.12/site-packages/nevergrad/optimization/utils.py:145\u001B[0m, in \u001B[0;36mDelayedJob.result\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mresult\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m tp\u001B[38;5;241m.\u001B[39mAny:\n\u001B[1;32m    144\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_computed:\n\u001B[0;32m--> 145\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    146\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_computed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    147\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result\n",
      "Cell \u001B[0;32mIn[75], line 35\u001B[0m, in \u001B[0;36mTrainManager.cost_function\u001B[0;34m(self, parameters)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcost_function\u001B[39m(\u001B[38;5;28mself\u001B[39m, parameters):\n\u001B[1;32m     33\u001B[0m     load_params(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, torch\u001B[38;5;241m.\u001B[39mtensor(parameters, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdouble))\n\u001B[0;32m---> 35\u001B[0m     test_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m#, accuracy = self.evaluate()\u001B[39;00m\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_score \u001B[38;5;241m>\u001B[39m test_loss :\n\u001B[1;32m     37\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_score \u001B[38;5;241m=\u001B[39m test_loss\n",
      "Cell \u001B[0;32mIn[75], line 23\u001B[0m, in \u001B[0;36mTrainManager.evaluate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     21\u001B[0m total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m x , y \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataloader_test :\n\u001B[0;32m---> 23\u001B[0m     yhat \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;66;03m#print(\"yhat\", yhat,\"y\", y)\u001B[39;00m\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;66;03m#correct += (torch.abs(yhat-y)<0.5).type(torch.double).sum().item() # Compute classification error\u001B[39;00m\n\u001B[1;32m     26\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss(yhat,y)\n",
      "File \u001B[0;32m~/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Opti_ML_Project/nevergrad/../models.py:125\u001B[0m, in \u001B[0;36mLSTM.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m--> 125\u001B[0m     embedded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    126\u001B[0m     output, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlstm(embedded)\n\u001B[1;32m    127\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc(output)\n",
      "File \u001B[0;32m~/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.venv/lib/python3.12/site-packages/torch/nn/modules/sparse.py:163\u001B[0m, in \u001B[0;36mEmbedding.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    162\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 163\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    164\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.venv/lib/python3.12/site-packages/torch/nn/functional.py:2264\u001B[0m, in \u001B[0;36membedding\u001B[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[1;32m   2258\u001B[0m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[1;32m   2259\u001B[0m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[1;32m   2260\u001B[0m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[1;32m   2261\u001B[0m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[1;32m   2262\u001B[0m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[1;32m   2263\u001B[0m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[0;32m-> 2264\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.DoubleTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "learned_param = optimizer.minimize(fitness)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

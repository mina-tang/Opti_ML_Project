{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader , Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "import nevergrad as ng\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from custom_datasets import WineDataset, LSTMDataset\n",
    "from utils import get_LSTM_dataloaders, isEnglish, lowerCase, build_vocabulary\n",
    "from models import CNN_Simple,All_CNN_C,VAE,LSTM,Simple_Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set device as GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to load the model parameters correctly\n",
    "def load_params(model, param_tensor):\n",
    "    current_index = 0\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param_length = param.numel()\n",
    "        param.data = param_tensor[current_index:current_index + param_length].reshape(param.size()).to(device)\n",
    "        current_index += param_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model\n",
    "class TrainManager:\n",
    "    def __init__(self, model, dataloader_train, dataloader_test, loss, device, isLSTM=False):\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device)\n",
    "        self.loss = loss.to(self.device)\n",
    "        self.best_model = None\n",
    "        self.dataloader_train = dataloader_train\n",
    "        self.dataloader_test = dataloader_test\n",
    "        self.best_score = 1e9\n",
    "        self.epoch = 1\n",
    "        self.n_samples = len(dataloader_train)\n",
    "        self.iteration = 0\n",
    "        self.test_losses = []\n",
    "        self.train_losses = []\n",
    "        self.accuracy = []\n",
    "        self.LSTM = isLSTM\n",
    "            \n",
    "    def evaluate(self):\n",
    "        total_correct,total_samples,loss = 0,0,0\n",
    "\n",
    "        if not self.LSTM:\n",
    "            for x , y in self.dataloader_test:\n",
    "                x,y = x.to(self.device),y.to(self.device)\n",
    "                y = y.flatten()\n",
    "\n",
    "                predicted = self.model(x)     \n",
    "                loss += self.loss(predicted, y).item() #Computed loss\n",
    "\n",
    "                predicted_labels = torch.argmax(predicted, axis=1) #Predicted labels for classifier problem\n",
    "                total_correct += torch.sum(y == predicted_labels).item() #Total number of correctly predicted labels\n",
    "                total_samples += y.numel()\n",
    "\n",
    "        else: #Special case for LSTM\n",
    "            for i, data in enumerate(self.dataloader_test):\n",
    "                inputs = data.to(self.device)\n",
    "                labels = inputs[:, 1:].to(self.device)\n",
    "\n",
    "                predicted = self.model(inputs).to(self.device)\n",
    "                predicted = predicted[:, :-1, :].permute(0, 2, 1)\n",
    "                loss += self.loss(predicted, labels).item()\n",
    "\n",
    "                predicted_labels = torch.argmax(predicted, axis=1).to(self.device)\n",
    "                total_correct += torch.sum(labels == predicted_labels).item()\n",
    "                total_samples += labels.numel()\n",
    "\n",
    "        return loss, total_correct/total_samples #Return loss and accuracy\n",
    "\n",
    "    def cost_function(self, parameters):\n",
    "        load_params(self.model, torch.tensor(parameters))\n",
    "\n",
    "        if not self.LSTM:\n",
    "            # Load the next batch\n",
    "            inputs, labels = next(iter(self.dataloader_train))\n",
    "            inputs,labels = inputs.to(self.device),labels.to(self.device)\n",
    "            predicted = self.model(inputs).to(self.device)\n",
    "            loss = self.loss(predicted, labels.flatten()).item()\n",
    "\n",
    "        else:\n",
    "                for i,data in enumerate(self.dataloader_train):\n",
    "                    if i == self.iteration % self.n_samples:\n",
    "                         inputs = data.to(self.device)\n",
    "                labels = inputs[:,1:].to(self.device)\n",
    "                predicted = self.model(inputs).to(self.device)\n",
    "                predicted = predicted[:, :-1, :].permute(0, 2, 1)\n",
    "                loss = self.loss(predicted, labels).item()\n",
    "\n",
    "        test_loss, accuracy = self.evaluate()\n",
    "        if self.best_score > test_loss : #update best score\n",
    "            self.best_score = test_loss\n",
    "            self.best_model = copy.deepcopy(self.model)\n",
    "\n",
    "        print(f'Epoch {self.epoch}, batch {self.iteration % self.n_samples +1}; test loss function : {test_loss}, accuracy: {accuracy:.2f}')\n",
    "\n",
    "        self.test_losses.append(test_loss)\n",
    "        self.train_losses.append(loss)\n",
    "        self.accuracy.append(accuracy)\n",
    "        \n",
    "        self.iteration += 1\n",
    "        if self.iteration % self.n_samples == 0: \n",
    "            self.epoch += 1\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To toggle optimizers, change all instances of NGOpt to SPSA and vice versa (including in file names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P0 : wine-dataset (small model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "\n",
    "#Load the dataset\n",
    "wine_quality = fetch_ucirepo(id=186) \n",
    "\n",
    "x = wine_quality.data.features \n",
    "y = wine_quality.data.targets\n",
    "data = pd.concat([x, y], axis=1)\n",
    "data_classes = np.sort(data[\"quality\"].unique())\n",
    "\n",
    "data_train, data_validation = train_test_split(data, random_state=104, test_size=0.25, shuffle=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "columns_to_normalize = data_train.columns[data_train.columns != \"quality\"].tolist()\n",
    "data_train[columns_to_normalize] = scaler.fit_transform(data_train[columns_to_normalize])\n",
    "data_validation[columns_to_normalize] = scaler.transform(data_validation[columns_to_normalize])\n",
    "\n",
    "dataset_train = WineDataset(data_train,data_classes)\n",
    "dataset_validation = WineDataset(data_validation,data_classes)\n",
    "training_loader = DataLoader(dataset_train,batch_size=len(dataset_train)//1,shuffle=True)\n",
    "validation_loader = DataLoader(dataset_validation,batch_size=len(dataset_validation)//1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model and loss function\n",
    "model = Simple_Net(len(data_classes)).to(device)\n",
    "torch.save(model.state_dict(), 'models/Simple_Net.pt')\n",
    "model.load_state_dict(torch.load('models/Simple_Net.pt'))\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart here for different optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set hyperparameters\n",
    "epochs = 100\n",
    "nb_batch = len(training_loader)\n",
    "trainer = TrainManager(model, training_loader, validation_loader, loss, device)\n",
    "fitness = trainer.cost_function\n",
    "\n",
    "# Compute number of parameters of the model + initialize parametrization\n",
    "init_params = torch.concatenate([p.data.flatten() for p in trainer.model.parameters()])\n",
    "parametrization = ng.p.Array(init=init_params.cpu())\n",
    "optimizer = ng.optimizers.SPSA(parametrization=parametrization, budget=epochs*nb_batch, num_workers=1)\n",
    "optimizer.a = 1\n",
    "print(f'{epochs} epochs; {nb_batch} batchs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimize the model\n",
    "learned_param = optimizer.minimize(fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the results\n",
    "plt.scatter(range(0,len(trainer.train_losses)),trainer.train_losses, label=\"training\", alpha=0.5)\n",
    "plt.scatter(range(0,len(trainer.test_losses)),trainer.test_losses, label=\"test\", alpha=0.5)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title(\"Training and testing loss as a function of epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(range(0,len(trainer.accuracy)), trainer.accuracy, label=\"accuracy\", alpha=0.5)\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title(\"accuracy as a function of epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the results\n",
    "torch.save(torch.tensor(trainer.test_losses), 'results/NGOpttestloss_P0.pt')\n",
    "torch.save(torch.tensor(trainer.train_losses), 'results/NGOpttrainloss_P0.pt')\n",
    "torch.save(torch.tensor(trainer.accuracy), 'results/NGOptaccuracy_P0.pt')\n",
    "optimizer._select_optimizer_cls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P1 : simple CNN for classification of Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "\n",
    "#Load the dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Lambda(lambda x: x.double()),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "training_set = torchvision.datasets.FashionMNIST(\"./data\", train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.FashionMNIST(\"./data\", train=False, transform=transform, download=True)\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=len(training_set)//2, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=len(validation_set)//2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model and loss function\n",
    "model = CNN_Simple().to(device)\n",
    "torch.save(model.state_dict(), 'models/CNN_Simple.pt')\n",
    "model.load_state_dict(torch.load('models/CNN_Simple.pt'))\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart here for different optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set hyperparameters\n",
    "epochs = 20\n",
    "nb_batch = len(training_loader)\n",
    "trainer = TrainManager(model, training_loader, validation_loader, loss, device)\n",
    "fitness = trainer.cost_function\n",
    "\n",
    "# Compute number of parameters of the model + initialize parametrization\n",
    "init_params = torch.concatenate([p.data.flatten() for p in trainer.model.parameters()])\n",
    "parametrization = ng.p.Array(init=init_params.cpu())\n",
    "optimizer = ng.optimizers.SPSA(parametrization=parametrization, budget=epochs*nb_batch, num_workers=1)\n",
    "optimizer.a = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimize the model\n",
    "learned_param = optimizer.minimize(fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the results\n",
    "plt.scatter(range(0,len(trainer.train_losses)), trainer.train_losses, label=\"training\", alpha=0.5)\n",
    "plt.scatter(range(0, len(trainer.test_losses)), trainer.test_losses, label=\"test\", alpha=0.5)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title(\"Training and testing loss as a function of epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(len(trainer.accuracy), trainer.accuracy, label=\"accuracy\", alpha=0.5)\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title(\"accuracy as a function of epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the results\n",
    "torch.save(torch.tensor(trainer.test_losses), 'results/NGOptloss_P1.pt')\n",
    "torch.save(torch.tensor(trainer.accuracy), 'results/NGOptaccuracy_P1.pt')\n",
    "optimizer._select_optimizer_cls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P3: VAE for generation on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "\n",
    "#Load the dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Lambda(lambda x: x.double()),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "training_set = torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=len(training_set)//100, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=len(validation_set)//100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model and loss function\n",
    "model = VAE().to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "model.load_state_dict(torch.load('models/VAE.pt'))\n",
    "torch.save(model.state_dict(), 'models/VAE.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart here for different optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set hyperparameters\n",
    "epochs = 1\n",
    "nb_batch = len(training_loader)\n",
    "trainer = TrainManager(model, training_loader, validation_loader, loss, device)\n",
    "fitness = trainer.cost_function\n",
    "\n",
    "# Compute number of parameters of the model + initialize parametrization\n",
    "init_params = torch.concatenate([p.data.flatten() for p in trainer.model.parameters()])\n",
    "parametrization = ng.p.Array(init=init_params.cpu())\n",
    "optimizer = ng.optimizers.NGOpt(parametrization=parametrization, budget=epochs*nb_batch, num_workers=1)\n",
    "optimizer.a = 1\n",
    "print(f'{epochs} epochs; {nb_batch} batchs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimize the model\n",
    "learned_param = optimizer.minimize(fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.scatter(range(0,len(trainer.train_losses)), trainer.train_losses, label=\"training\", alpha=0.5)\n",
    "plt.scatter(range(0,len(trainer.test_losses)), trainer.test_losses, label=\"test\", alpha=0.5)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title(\"Training and testing loss as a function of epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(range(0,trainer.accuracy), trainer.accuracy, label=\"accuracy\", alpha=0.5)\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title(\"accuracy as a function of epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the results\n",
    "torch.save(torch.tensor(trainer.test_losses), 'results/NGOptloss_P1.pt')\n",
    "torch.save(torch.tensor(trainer.accuracy), 'results/NGOptaccuracy_P1.pt')\n",
    "optimizer._select_optimizer_cls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P4: All-CNNC-C for classification on CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.double()),\n",
    "    transforms.Normalize((0.5074,0.4867,0.4411),(0.2011,0.1987,0.2025))]\n",
    ")\n",
    "\n",
    "#Load the dataset\n",
    "training_set = torchvision.datasets.CIFAR100('./data', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.CIFAR100('./data', train=False, transform=transform, download=True)\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=len(training_set)//50, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=len(validation_set)//50, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model and loss function\n",
    "model = All_CNN_C().to(device)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "model.load_state_dict(torch.load('models/All_CNN_C.pt'))\n",
    "torch.save(model.state_dict(), 'models/All_CNN_C.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "restart here for evaluating with different optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set hyperparameters\n",
    "epochs = 1\n",
    "nb_batch = len(training_loader)\n",
    "trainer = TrainManager(model, training_loader, validation_loader, loss, device)\n",
    "fitness = trainer.cost_function\n",
    "\n",
    "# Compute number of parameters of the model + initialize parametrization\n",
    "init_params = torch.concatenate([p.data.flatten() for p in trainer.model.parameters()])\n",
    "parametrization = ng.p.Array(init=init_params)\n",
    "optimizer = ng.optimizers.NGOpt(parametrization=parametrization, budget=epochs*nb_batch, num_workers=1)\n",
    "optimizer.a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimize the model\n",
    "learned_param = optimizer.minimize(fitness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P5: next word generation (2-layer bidirectional LSTM trained on wikitext-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the results\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"wikitext\", 'wikitext-2-v1', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the data\n",
    "import re\n",
    "dataset = dataset.filter(lambda x: 100 <= len(x['text'].split()) <= 128)\n",
    "dataset = dataset.filter(lambda x: not re.match(\" = .* = \\n\", x['text']))\n",
    "dataset = dataset.filter(lambda x: isEnglish(x['text']))\n",
    "dataset = dataset.map(lambda x: lowerCase(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the vocabulary\n",
    "wikitext_dataset, token_freq_dict = build_vocabulary(dataset, min_freq=5, unk_token='<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define LSTM Dataset Class\n",
    "class LSTMDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 dataset: datasets.arrow_dataset.Dataset,\n",
    "                 max_seq_length: int, ):\n",
    "        self.train_data = self.prepare_dataset(dataset)\n",
    "        self.max_seq_length = max_seq_length + 2  # as <start> and <stop> will be added\n",
    "        self.dataset_vocab = self.get_vocabulary(dataset)\n",
    "        self.token2idx = {element: index for index, element in enumerate(self.dataset_vocab)}\n",
    "        self.idx2token = dict(enumerate(self.dataset_vocab))\n",
    "        self.pad_idx = self.token2idx[\"<pad>\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get a list of tokens of the given sequence. Represent each token with its index in `self.token2idx`.\n",
    "        token_list = self.train_data[idx].split()\n",
    "        # having a fallback to <unk> token if an unseen word is encoded.\n",
    "        token_ids = [self.token2idx.get(t, self.token2idx['<unk>']) for t in token_list]\n",
    "\n",
    "        # Add padding token to the sequence to reach the max_seq_length. \n",
    "        token_ids += [self.token2idx['<pad>']] * (self.max_seq_length - len(token_ids))\n",
    "\n",
    "        return torch.tensor(token_ids)\n",
    "\n",
    "    def get_vocabulary(self, dataset: datasets.arrow_dataset.Dataset):\n",
    "        vocab = set()\n",
    "        print(\"Getting dataset's vocabulary\")\n",
    "        for sample in tqdm(dataset):\n",
    "            vocab.update(set(sample[\"text\"].split()))\n",
    "        vocab.update(set([\"<start>\", \"<stop>\", \"<pad>\"]))\n",
    "        vocab = sorted(vocab)\n",
    "        return vocab\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_dataset(target_dataset: datasets.arrow_dataset.Dataset):\n",
    "        \"\"\"\n",
    "        Encapsulate sequences between <start> and <stop>.\n",
    "        \n",
    "        :param: target_dataset: the target dataset to extract samples\n",
    "        return: a list of encapsulated samples.\n",
    "        \"\"\"\n",
    "        prepared_dataset = []\n",
    "        for sample in target_dataset:\n",
    "            prepared_dataset.append(f\"<start> {sample['text']} <stop>\")\n",
    "        return prepared_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 128\n",
    "lstm_dataset = LSTMDataset(dataset=wikitext_dataset,\n",
    "                         max_seq_length=MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "training_loader, validation_loader = get_LSTM_dataloaders(lstm_dataset, test_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(lstm_dataset.token2idx)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 100\n",
    "dropout_rate = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model and loss function\n",
    "model = LSTM(vocab_size=vocab_size,input_dim=embedding_dim,hidden_dim=hidden_dim,dropout_rate=dropout_rate).to(device)\n",
    "torch.save(model.state_dict(), 'models/LSTM.pt')\n",
    "model.load_state_dict(torch.load('models/LSTM.pt'))\n",
    "loss = torch.nn.CrossEntropyLoss(ignore_index=lstm_dataset.pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "restart here for evaluating different optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set hyperparameters\n",
    "epochs = 2\n",
    "trainer = TrainManager(model,training_loader,validation_loader,loss,device,True)\n",
    "fitness = trainer.cost_function\n",
    "print(f'{epochs} epochs, {len(training_loader)} batchs')\n",
    "# Compute number of parameters of the model + initialize parametrization\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "parametrization = ng.p.Array(shape=(num_params,))\n",
    "optimizer = ng.optimizers.NGOpt(parametrization=parametrization, budget=epochs*len(training_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimize the model\n",
    "learned_param = optimizer.minimize(fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(0,trainer.train_losses), trainer.train_losses, label=\"training\", alpha=0.5)\n",
    "plt.scatter(range(0,trainer.test_losses), trainer.test_losses, label=\"test\", alpha=0.5)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title(\"Training and testing loss as a function of epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(range(0,trainer.accuracy), trainer.accuracy, label=\"accuracy\", alpha=0.5)\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title(\"accuracy as a function of epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the results\n",
    "torch.save(torch.tensor(loss), 'results/NGOptloss_P5V2.pt')\n",
    "torch.save(torch.tensor(accuracy), 'results/NGOptaccuracy_P5V2.pt')\n",
    "optimizer._select_optimizer_cls()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
